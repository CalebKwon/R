# https://www.r-bloggers.com/how-to-perform-pca-with-r/
# csv 파일 불러오기 (file importing)
secu_com_finance<- read.csv("~/Dropbox/Statlecture/24H/data/finance.csv",header = TRUE,stringsAsFactors = FALSE,fileEncoding="euc-kr")

# V1 : 총자본순이익율
# V2 : 자기자본순이익율
# V3 : 자기자본비율
# V4 : 부채비율
# V5 : 자기자본회전율

# 표준화 변환 (standardization)
secu_com_finance<- transform(secu_com_finance, 
                                    V1_s = scale(V1), 
                                    V2_s = scale(V2), 
                                    V3_s = scale(V3), 
                                    V4_s = scale(V4),
                                    V5_s = scale(V5))

# V1.총자본순이익율, V2.자기자본순이익율, V3.자기자본비율, V5.자기자본회전율의 네 개의 변수는 숫자가 클 수록 좋다는 뜻이지만 V4.부채비율는 높을 수록 안 좋다고 해석하게 됩니다.  
# 즉 V1, V2, V3, V5와 V4는 반대방향으로 움직이게 되는데요, 서로 같은 방향으로 움직이게 해서 상관도가 높게 나와 같은 주성분에 반영되도록 하기 위해서 아래와 같이 V4.
# 부채비율의 방향을 변환

# 부채비율(V4_s)을 방향(max(V4_s)-V4_s) 변환
secu_com_finance<- transform(secu_com_finance, 
                                    V4_s2 = max(V4_s) - V4_s)
 
# variable selection
secu_com_finance<- secu_com_finance[,c("company", "V1_s", "V2_s", "V3_s", "V4_s2", "V5_s")]

# Correlation analysis
cor(secu_com_finance[,-1])

round(cor(secu_com_finance[,-1]), digits=3) # 반올림

# Scatter plot matrix
plot(secu_com_finance[,-1])

# 주성분분석 PCA(Principal Component Analysis)
secu_prcomp <- prcomp(secu_com_finance[,c(2:6)]) # 첫번째 변수 회사명은 빼고 분석
summary(secu_prcomp)

# 
print(secu_prcomp)

# Scree Plot
plot(prcomp(secu_com_finance[,c(2:6)]), type="l",sub = "Scree Plot")

# Biplot
biplot(prcomp(secu_com_finance[,c(2:6)]), cex = c(0.7, 0.8))
 
# 관측치별 주성분1, 주성분2 점수 계산(PC1 score, PC2 score)
secu_pc1 <- predict(secu_prcomp)[,1]
secu_pc2 <- predict(secu_prcomp)[,2]
 
# 관측치별 이름 매핑(rownames mapping)
text(secu_pc1, secu_pc2, labels = secu_com_finance$company,cex = 0.7, pos = 3, col = "blue")

# PCA 두번째 예제
data(iris)
iris
iris$Species<-factor(iris$Species,levels=c("versicolor","Virginica","setosa"))
str(iris)
round(cor(iris[,1:4]),2)
pc<-princomp(iris[,1:4],cor=TRUE,scores=TRUE)
summary(pc)
plot(pc,type="lines")

biplot(pc,cex = c(0.7, 0.8))

# 관측치별 주성분1, 주성분2 점수 계산(PC1 score, PC2 score)
# iris_pc1 <- predict(pc)[,1]
# iris_pc2 <- predict(pc)[,2]
 
# 관측치별 이름 매핑(rownames mapping)
# text(iris_pc1, iris_pc2, labels = iris$Species, cex = 0.7, pos = 3, col = "blue")


#  세번째 예제
#  hsb2 data : labels=SES, Female
#  iris data : labels=class   
#  medical data : labels= age1(age를 3등급으로 나눠서,young,mid,old)
#  mtcars : label 
#  UScereal : label = mfr or # 이전에 그렸던 scatter plot과 어떤 차이점이 있을까?
#  리스트에서 원하는 데이터 셋을 하나 찾아서 MDS분석을 하세요.
#  https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html
#  예 ) state.x77 : 미국의 주별 환경


# http://rpubs.com/sinhrks/plot_pca
# {ggfortify} let {ggplot2} know how to interpret PCA objects. After loading {ggfortify}, 
# you can use ggplot2::autoplot function forstats::prcomp and stats::princomp objects.
install.packages('ggfortify')
library(ggfortify)
df <- iris[c(1, 2, 3, 4)]
autoplot(prcomp(df))
autoplot(prcomp(df), data = iris, colour = 'Species')

# Label = TRUE draws each data label using rownames
autoplot(prcomp(df), data = iris, colour = 'Species', label = TRUE, label.size = 3)

# shape = FALSE makes plot without points. In this case, label is turned on unless otherwise specified.
autoplot(prcomp(df), data = iris, colour = 'Species', shape = FALSE, label.size = 3)

# loadings = TRUE draws eigenvectors.
autoplot(prcomp(df), data = iris, colour = 'Species', loadings = TRUE)

# You can attach eigenvector labels and change some options.
autoplot(prcomp(df), data = iris, colour = 'Species',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)

# K-means clustering
autoplot(kmeans(USArrests, 3), data = USArrests)

# 
autoplot(kmeans(USArrests, 3), data = USArrests, label = TRUE, label.size = 3)

## clustering
## It is based on some notion of "distance" (the inverse of similarity) between data points and use that to identify data points that are close-by to each other.
## http://www.instantr.com/2013/02/12/performing-a-cluster-analysis-in-r/
## http://horicky.blogspot.kr/2012/04/machine-learning-in-r-clustering.html

##	1.	Place K points into the space represented by the objects that are being clustered. These points represent initial group centroids.
##	2.	Assign each object to the group that has the closest centroid.
##	3.	When all objects have been assigned, recalculate the positions of the K centroids.
##	4.	Repeat Steps 2 and 3 until the centroids no longer move. This produces a separation of the objects into groups from which the metric to be minimized can be calculated.

# https://rstudio-pubs-static.s3.amazonaws.com/33876_1d7794d9a86647ca90c4f182df93f0e8.html
# install.packages('rattle')
install.packages("rattle")
data(wine, package='rattle')
head(wine)
wine[-1]

# wine data set
# https://onlinecourses.science.psu.edu/stat857/node/223

wine.stand <- scale(wine[-1])  # To standarize the variables
k.means.fit <- kmeans(wine.stand, 3)

attributes(k.means.fit)
k.means.fit$cluster

# Cluster size:
k.means.fit$size

# the percentage of variance explained by the clusters against the number of clusters
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(wine.stand, nc=6) 

library(cluster)
clusplot(wine.stand, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)


Hierarchical clustering:
# wine data
d <- dist(wine.stand, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward") # Ward’s minimum variance criterion minimizes the total within-cluster variance
plot(H.fit) # display dendogram
groups <- cutree(H.fit, k=3) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters
rect.hclust(H.fit, k=3, border="red")

# USarrests data
df <- USArrests
df <- na.omit(df)
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)


# European protein consumption
url = 'http://www.biz.uiowa.edu/faculty/jledolter/DataMining/protein.csv'
food <- read.csv(url)
head(food)
grpMeat <- kmeans(food[,c("WhiteMeat","RedMeat")], centers=3, nstart=10)
grpMeat

## list of cluster assignments
o=order(grpMeat$cluster)
data.frame(food$Country[o],grpMeat$cluster[o])

# To get all the records belonging to cluster 1:
# data_clustered <- kmeans(data)
# data_clustered$cluster
# data$cluster <- data_clustered$cluster 
# data_clus_1 <- data[data$cluster == 1,]
plot(food$Red, food$White, type="n", xlim=c(3,19), xlab="Red Meat", ylab="White Meat")
text(x=food$Red, y=food$White, labels=food$Country,col=grpMeat$cluster+1)

groups <- cutree(foodagg, k=4) # cut tree into 3 clusters
rect.hclust(foodagg, k=4, border="red")


# http://www.learnbymarketing.com/tutorials/k-means-clustering-in-r-example/
# 1) FRESH: annual spending (m.u.) on fresh products (Continuous); 
# 2) MILK: annual spending (m.u.) on milk products (Continuous); 
# 3) GROCERY: annual spending (m.u.)on grocery products (Continuous); 
# 4) FROZEN: annual spending (m.u.)on frozen products (Continuous) 
# 5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) 
# 6) DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous); 
# 7) CHANNEL: customersâ€™ Channel - Horeca (Hotel/Restaurant/CafÃ©) or Retail channel (Nominal) 
# 8) REGION: customersâ€™ Region â€“ Lisnon, Oporto or Other (Nominal) 


